{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mouraffa/KNN-From-Scratch-Iris-Classifier/blob/main/KNN_Explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c9897f",
      "metadata": {
        "id": "79c9897f"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eba9cee4",
      "metadata": {
        "id": "eba9cee4"
      },
      "source": [
        "\n",
        "üìå **K-nearest neighbors (KNN)** is a type of supervised learning algorithm used for both regression and classification. KNN tries to predict the correct class for the test data by calculating the distance between the test data and all the training points. Then it selects the K number of points which are closest to the test data. The KNN algorithm calculates the probability of the test data belonging to the classes of ‚ÄòK‚Äô training data and the class that holds the highest probability will be selected. In the ca...\n",
        "\n",
        "üê±üê∂ Suppose we have an image of a creature that looks similar to a cat and dog, but we want to know whether it is a cat or dog. We can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new dataset to the cats and dogs images and based on the most similar features, it will categorize it as either a cat or dog.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18dd44dc",
      "metadata": {
        "id": "18dd44dc"
      },
      "source": [
        "# Why do we need a K-NN Algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ad5055",
      "metadata": {
        "id": "e0ad5055"
      },
      "source": [
        "\n",
        "Suppose there are two categories, i.e., Category A and Category B, and we have a new data point x1, so this data point will lie in which of these categories. To solve this type of problem, we need a K-NN algorithm. With the help of K-NN, we can easily identify the category or class of a particular dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b7bf7b5",
      "metadata": {
        "id": "4b7bf7b5"
      },
      "source": [
        "# How does K-NN work?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52f0f0ae",
      "metadata": {
        "id": "52f0f0ae"
      },
      "source": [
        "\n",
        "The K-NN working can be explained based on the following algorithm:\n",
        "\n",
        "1. üßÆ Select the number K of the neighbors.\n",
        "2. üìê Calculate the Euclidean distance of K number of neighbors.\n",
        "3. üìè Take the K nearest neighbors as per the calculated Euclidean distance.\n",
        "4. üìä Among these k neighbors, count the number of the data points in each category.\n",
        "5. üè∑Ô∏è Assign the new data points to that category for which the number of the neighbor is maximum.\n",
        "6. ‚úÖ Our model is ready.\n",
        "\n",
        "For a new data point, firstly, we will choose the number of neighbors, for instance, k=5. Next, we will calculate the Euclidean distance between the data points. This distance is the distance between two points, which is a concept from geometry. The Euclidean distance between two points \\( P(x_1, y_1) \\) and \\( Q(x_2, y_2) \\) in a 2D plane is given by:\n",
        "\n",
        "\\[\n",
        "d(P, Q) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n",
        "\\]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45341b48",
      "metadata": {
        "id": "45341b48"
      },
      "source": [
        "# How to choose a K value?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad401aab",
      "metadata": {
        "id": "ad401aab"
      },
      "source": [
        "\n",
        "The K value indicates the count of the nearest neighbors. We have to compute distances between test points and trained label points. Updating distance metrics with every iteration is computationally expensive, hence KNN is termed a lazy learning algorithm.\n",
        "\n",
        "Choosing an optimal K value is essential:\n",
        "\n",
        "- There are no pre-defined statistical methods to find the most favorable value of K.\n",
        "- Initialize a random K value and start computing.\n",
        "- A smaller value of K leads to unstable decision boundaries.\n",
        "- A larger K value is better for classification as it leads to smoothening the decision boundaries.\n",
        "- Derive a plot between the error rate and K denoting values in a defined range. Then choose the K value having a minimum error rate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df9f1884",
      "metadata": {
        "id": "df9f1884"
      },
      "source": [
        "# Calculating distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e66b1a7c",
      "metadata": {
        "id": "e66b1a7c"
      },
      "source": [
        "\n",
        "The first step is to calculate the distance between the new point and each training point. There are various methods for calculating this distance, of which the most commonly known methods are:\n",
        "\n",
        "- **Euclidean Distance**: Calculated as the square root of the sum of the squared differences between a new point \\( x \\) and an existing point \\( y \\). It can be represented mathematically as:\n",
        "\n",
        "\\[\n",
        "d(x, y) = \\sqrt{\\sum_{i=1}^{n} (y_i - x_i)^2}\n",
        "\\]\n",
        "\n",
        "- **Manhattan Distance**: The distance between real vectors using the sum of their absolute difference. It's given by:\n",
        "\n",
        "\\[\n",
        "d(x, y) = \\sum_{i=1}^{n} \\left| y_i - x_i\n",
        "ight|\n",
        "\\]\n",
        "\n",
        "- **Hamming Distance**: Used for categorical variables. If the value \\( x \\) and the value \\( y \\) are the same, the distance \\( D \\) will be equal to 0. Otherwise, \\( D=1 \\).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}